{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(class_):\n",
    "    dataFrames = []\n",
    "    for i in range(1, 8):\n",
    "        df = pd.read_csv('datasets/' + class_ + '.Cleaned.k' + str(i) + '.csv')\n",
    "        dataFrames.append(df)\n",
    "    return dataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chiroptera Class: k=1, 2, ...., 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chirop_dfs = read_csv('Chiroptera')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rodentia Class: k=1, 2, ...., 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rodent_dfs = read_csv('Rodentia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aves Class: k=1, 2, ...., 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aves_dfs = read_csv('Aves')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1_scores(X, y):\n",
    "    skf = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
    "    f1_scores = {'svmrad test': [], 'svmrad train': [], 'svmlin test': [], 'svmlin train': [], 'rf test': [], 'rf train': []}\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "        y_train, y_test = y.iloc[train_index, :], y.iloc[test_index, :]\n",
    "        svm_rad = SVC(gamma=1/len(X_train.index), kernel='rbf')\n",
    "        svm_lin = SVC(gamma=1/len(X_train.index), kernel='linear')\n",
    "        rf = RandomForestClassifier(n_estimators=10)\n",
    "        svm_rad.fit(X_train, y_train)\n",
    "        svm_lin.fit(X_train, y_train)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred_rad_test = svm_rad.predict(X_test)\n",
    "        y_pred_rad_train = svm_rad.predict(X_train)\n",
    "        y_pred_lin_test = svm_lin.predict(X_test)\n",
    "        y_pred_lin_train = svm_lin.predict(X_train)\n",
    "        y_pred_rf_test = rf.predict(X_test)\n",
    "        y_pred_rf_train = rf.predict(X_train)\n",
    "        f1_scores['svmrad test'].append(f1_score(y_test, y_pred_rad_test, average='micro'))\n",
    "        f1_scores['svmrad train'].append(f1_score(y_train, y_pred_rad_train, average='micro'))\n",
    "        f1_scores['svmlin test'].append(f1_score(y_test, y_pred_lin_test, average='micro'))\n",
    "        f1_scores['svmlin train'].append(f1_score(y_train, y_pred_lin_train, average='micro'))\n",
    "        f1_scores['rf test'].append(f1_score(y_test, y_pred_rf_test, average='micro'))\n",
    "        f1_scores['rf train'].append(f1_score(y_train, y_pred_rf_train, average='micro'))\n",
    "    return f1_scores        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(class_, name):\n",
    "    clfs_acc = {}\n",
    "    for i in range(1, len(class_)+1):\n",
    "        X = class_[i].iloc[:, 3:]\n",
    "        features = X.columns\n",
    "        sc = StandardScaler()\n",
    "        X = sc.fit_transform(X)\n",
    "        y = pd.DataFrame(.iloc[:, 2])\n",
    "        X = pd.DataFrame(X, columns=features)\n",
    "        clfs_acc[name+'.k'+str(i)] = get_f1_scores(X, y)\n",
    "    return clfs_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dbm.open('non-hierarchical f1_scores', 'c')\n",
    "db['Chiroptera'] = pickle.dumps(main(chirop_dfs, 'Chiroptera'))\n",
    "db['Rodentia'] = pickle.dumps(main(rodent_dfs, 'Rodentia'))\n",
    "db['Aves'] = pickle.dumps(main(aves_dfs, 'Aves'))\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dataset():\n",
    "    dataFrames = []\n",
    "    for i in range(1, 8):\n",
    "        chirop_df = pd.read_csv('datasets/Chiroptera.Cleaned.k' + str(i) + '.csv')\n",
    "        rodent_df = pd.read_csv('datasets/Rodentia.Cleaned.k' + str(i) + '.csv')\n",
    "        aves_df = pd.read_csv('datasets/Aves.Cleaned.k' + str(i) + '.csv')\n",
    "        frames = [chirop_df, rodent_df, aves_df]\n",
    "        merged_df = pd.concat(frames)\n",
    "        merged_df.index = range(len(merged_df))\n",
    "        dataFrames.append(merged_df)\n",
    "    return dataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dfs = combine_dataset()\n",
    "db = dbm.open('merged dataset', 'c')\n",
    "db['merged datasets'] = pickle.dumps(main(merged_dfs, 'Merged Datasets'))\n",
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
